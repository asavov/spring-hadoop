<beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:batch="http://www.springframework.org/schema/batch" xmlns:hdp="http://www.springframework.org/schema/hadoop"
	xmlns:c="http://www.springframework.org/schema/c" xmlns:p="http://www.springframework.org/schema/p"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
      	http://www.springframework.org/schema/batch	http://www.springframework.org/schema/batch/spring-batch.xsd
      	http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd">

	<import resource="../batch-common.xml" />
	<import resource="../hadoop-ctx.xml" />

	<hdp:script id="cleanScript" language="javascript" run-at-startup="true">
		if
		(fsh.test("${hdfs.item.writer.output.dir}")) {
		fsh.rmr("${hdfs.item.writer.output.dir}")
		}
	</hdp:script>

	<hdp:configuration id="seqFileHadoopConfiguration" configuration-ref="hadoopConfiguration">
		io.seqfile.compress.blocksize=1024
	</hdp:configuration>

	<!-- Create SeqFile serialization format -->
	<bean abstract="true" id="sequenceFileFormat" class="org.springframework.data.hadoop.serialization.SequenceFileFormat">
		<constructor-arg value="org.springframework.data.hadoop.serialization.HdfsWriterTest.PojoWritable" />

		<property name="configuration" ref="seqFileHadoopConfiguration" />
		<property name="hdfsResourceLoader" ref="hadoopResourceLoader" />
	</bean>

	<!-- Create AVRO serialization format -->
	<bean abstract="true" id="avroFormat" class="org.springframework.data.hadoop.serialization.AvroFormat">
		<constructor-arg value="org.springframework.data.hadoop.serialization.HdfsWriterTest.PojoWritable" />

		<property name="hdfsResourceLoader" ref="hadoopResourceLoader" />
	</bean>

	<!-- Reference to used serialization format: SeqFile or Avro. -->
	<!-- compressionAlias: none, deflate -->
	<bean id="serializationFormatRef" parent="sequenceFileFormat" p:compressionAlias="none" />

	<!-- Create Serialization Writer factory -->
	<bean id="swObjectFactory" class="org.springframework.data.hadoop.serialization.SerializationWriterObjectFactory">
		<constructor-arg ref="hadoopResourceLoader" />

		<property name="serializationFormat" ref="serializationFormatRef" />
	</bean>

	<!-- Configure single-step jobs writing to hdfs using different Writers and Completion policies -->

	<bean class="org.springframework.batch.core.scope.StepScope" p:proxyTargetClass="true" />

	<bean id="completionPolicy" class="org.springframework.batch.repeat.policy.DefaultResultCompletionPolicy" />

	<!-- Provide original objects to write -->
	<bean id="pojosItemReader"
		class="org.springframework.data.hadoop.batch.HdfsSerializationFormatWriteReadTest.ObjectsReader"
		p:objectsCount="100" scope="step" />

	<!-- Store objects from HDFS -->
	<bean id="pojosItemWriter"
		class="org.springframework.data.hadoop.batch.HdfsSerializationFormatWriteReadTest.ObjectsWriter"
		scope="step" />

	<!-- HdfsItemWriter: writes all POJOs to a single destination -->

	<!-- Write original objects to HDFS -->
	<bean id="hdfsItemWriter" class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemWriter"
		p:serializationFormat-ref="swObjectFactory" p:location="${hdfs.item.writer.output.dir}hdfsItemWriter"
		scope="step">
	</bean>

	<!-- Read objects from HDFS -->
	<bean id="hdfsItemReader" class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemReader"
		p:serializationFormat-ref="serializationFormatRef" p:location="${hdfs.item.writer.output.dir}hdfsItemWriter"
		scope="step">
	</bean>

	<job id="hdfsItemWriterJob" xmlns="http://www.springframework.org/schema/batch">
		<step id="hdfsItemWriterStep" next="hdfsItemReaderStep">
			<tasklet>
				<chunk reader="pojosItemReader" writer="hdfsItemWriter" chunk-completion-policy="completionPolicy" />
			</tasklet>
		</step>
		<step id="hdfsItemReaderStep">
			<tasklet>
				<chunk reader="hdfsItemReader" writer="pojosItemWriter" chunk-completion-policy="completionPolicy" />
			</tasklet>
		</step>
	</job>

	<!-- Restartable HDFS Item Reader -->

	<!-- Write original objects to HDFS -->
	<bean id="restartableReaderTest_ItemWriter" class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemWriter"
		p:serializationFormat-ref="swObjectFactory" p:location="${hdfs.item.writer.output.dir}restartableReaderTest"
		scope="step">
	</bean>

	<!-- Read objects from HDFS -->
	<bean id="restartableReaderTest_ItemReader"
		class="org.springframework.data.hadoop.batch.HdfsSerializationFormatWriteReadTest.FailingHdfsSerializationFormatItemReaderTest"
		p:serializationFormat-ref="serializationFormatRef" p:location="${hdfs.item.writer.output.dir}restartableReaderTest"
		scope="prototype" />

	<job id="restartableReaderTest" xmlns="http://www.springframework.org/schema/batch">
		<step id="restartableReaderTest_WriteStep">
			<tasklet>
				<chunk reader="pojosItemReader" writer="restartableReaderTest_ItemWriter"
					chunk-completion-policy="completionPolicy" />
			</tasklet>
		</step>
	</job>

	<!-- HdfsItemStreamWriter: writes all POJOs in chunks of 25 to a single destination -->

	<!-- Write original objects to HDFS -->
	<bean id="hdfsItemStreamWriter" class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemStreamWriter"
		p:serializationFormat-ref="swObjectFactory" p:location="${hdfs.item.writer.output.dir}hdfsItemStreamWriter"
		scope="step">
	</bean>

	<!-- Read objects from HDFS -->
	<bean id="hdfsItemStreamReader" class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemReader"
		p:serializationFormat-ref="serializationFormatRef" p:location="${hdfs.item.writer.output.dir}hdfsItemStreamWriter"
		scope="step">
	</bean>

	<job id="hdfsItemStreamWriterJob" xmlns="http://www.springframework.org/schema/batch">
		<step id="hdfsItemStreamWriterStep" next="hdfsItemStreamReaderStep">
			<tasklet>
				<chunk reader="pojosItemReader" writer="hdfsItemStreamWriter" commit-interval="25" />
			</tasklet>
		</step>
		<step id="hdfsItemStreamReaderStep">
			<tasklet>
				<chunk reader="hdfsItemStreamReader" writer="pojosItemWriter" commit-interval="25" />
			</tasklet>
		</step>
	</job>


	<!-- HdfsMultiResourceItemWriter: writes every chunk of 25 POJOs to a separate destination -->

	<!-- Write original objects to HDFS -->
	<bean id="hdfsMultiResourceItemWriter"
		class="org.springframework.data.hadoop.batch.HdfsSerializationFormatMultiResourceItemWriter"
		p:delegate-ref="hdfsItemWriter" p:baseLocation="${hdfs.item.writer.output.dir}hdfsMultiResourceItemWriter"
		scope="step">
	</bean>

	<!-- Read objects from HDFS -->
	<bean id="hdfsMultiResourceItemReader" class="org.springframework.data.hadoop.batch.HdfsMultiResourceItemReader"
		p:locationPattern="${hdfs.item.writer.output.dir}hdfsMultiResourceItemWriter.*" scope="step">
		<property name="delegate">
			<bean class="org.springframework.data.hadoop.batch.HdfsSerializationFormatItemReader"
				p:serializationFormat-ref="serializationFormatRef" />
		</property>
		<property name="hdfsResourceLoader" ref="hadoopResourceLoader" />
	</bean>

	<job id="hdfsMultiResourceItemWriterJob" xmlns="http://www.springframework.org/schema/batch">
		<step id="hdfsMultiResourceItemWriterStep" next="hdfsMultiResourceItemReaderStep">
			<tasklet>
				<chunk reader="pojosItemReader" writer="hdfsMultiResourceItemWriter" commit-interval="25" />
			</tasklet>
		</step>
		<step id="hdfsMultiResourceItemReaderStep">
			<tasklet>
				<chunk reader="hdfsMultiResourceItemReader" writer="pojosItemWriter"
					chunk-completion-policy="completionPolicy" />
			</tasklet>
		</step>
	</job>
</beans>